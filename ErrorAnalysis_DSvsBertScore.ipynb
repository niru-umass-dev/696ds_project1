{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3xaTKXlsEfM3HxRFJ1dtT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Error-Analysis of Outliers of (ds_ref_base vs ds_ref_paraphrase) and (bs_ref_base vs bs_ref_paraphrase)"],"metadata":{"id":"vc3YydyAssY1"}},{"cell_type":"code","source":["#Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","FOLDERNAME = 'CS696DS/696ds_project1'\n","%cd /content/drive/My\\Drive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xB4QXBO9oOLc","executionInfo":{"status":"ok","timestamp":1682050168852,"user_tz":240,"elapsed":19703,"user":{"displayName":"Philip George","userId":"01883735276937380273"}},"outputId":"32b3522e-03bf-4797-d5cb-579205bc224e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/CS696DS/696ds_project1\n"]}]},{"cell_type":"code","source":["!pip install py-rouge"],"metadata":{"id":"va5XqLK9AKEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from statistics import stdev\n","from scipy.stats import spearmanr\n","from scipy import stats\n","import numpy as np\n","import rouge\n","from collections import Counter\n","import nltk\n","import json\n","nltk.download('punkt')"],"metadata":{"id":"5kwdTLLfs8z6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682059848394,"user_tz":240,"elapsed":322,"user":{"displayName":"Philip George","userId":"01883735276937380273"}},"outputId":"70930b6e-e117-48d7-d094-4ffdab6ea177"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["analysis_dict = {}\n","## HELPER FUNCTIONS\n","def get_evaluator():\n","    return rouge.Rouge(metrics=[\"rouge-n\", \"rouge-l\"], max_n=2, limit_length=False, apply_avg=True, stemming=True, ensure_compatibility=True)\n","    \n","def stem(x, evaluator):\n","    return Counter(evaluator.stem_tokens(evaluator.tokenize_text(x.lower())))\n","\n","def calc_ds(summ_a, summ_b, summ_comm, evaluator):\n","    s_a, s_b, s_c = stem(summ_a, evaluator), stem(summ_b, evaluator), stem(summ_comm, evaluator)\n","    nr = sum((s_a & s_b).values()) + sum((s_a & s_c).values()) + sum((s_b & s_c).values()) - 2.0 * sum((s_a & s_b & s_c).values())\n","    dr = sum((s_a | s_b | s_c).values())\n","    return 1.0 - (nr / dr)\n","\n","def get_centtend_measures(x, y):\n","    fig, ax1 = plt.subplots(1)\n","    x = [100*value for value in x]\n","    y = [100*value for value in y]\n","    ax1.scatter(x, y)\n","    return np.mean(y), np.std(y), spearmanr(x,y).statistic, np.corrcoef(x,y)[0,1], fig\n","\n","def get_count_change(count_1, count_2, sort=True):\n","  count_1 = {k:0 for k in (count_1 | count_2).keys()} | count_1\n","  delta = {k:int(count_2[k] - v) for k,v in count_1.items()}\n","  key_types = set(map(lambda x: type(x), delta.keys()))\n","  value_types = set(map(lambda x: type(x), delta.values()))\n","  delta = dict(sorted(delta.items(), key=lambda x: np.abs(x[1]), reverse=True))\n","  return delta\n","\n","def perform_analysis(s, p, s_ds, p_ds, s_bs, p_bs, summ_types, reason):\n","  s_a, s_b, s_c = stem(s[0], evaluator), stem(s[1], evaluator), stem(s[2], evaluator)\n","  s_a_b, s_a_c, s_b_c  = s_a & s_b, s_a & s_c, s_b & s_c\n","  s_a_b_c = s_a & s_b & s_c\n","  s_abc = s_a | s_b | s_c\n","  \n","  p_a, p_b, p_c = stem(p[0], evaluator), stem(p[1], evaluator), stem(p[2], evaluator)\n","  p_a_b, p_a_c, p_b_c, p_a_b_c = p_a & p_b, p_a & p_c, p_b & p_c, p_a & p_b & p_c\n","  p_a_b_c = p_a & p_b & p_c\n","  p_abc = p_a | p_b | p_c\n","\n","  a_b = get_count_change(s_a_b, p_a_b)\n","  a_c = get_count_change(s_a_c, p_a_c)\n","  b_c = get_count_change(s_b_c, p_b_c)\n","  a_b_c = get_count_change(s_a_b_c, p_a_b_c)\n","  abc = get_count_change(s_abc, p_abc)\n","\n","  print(f\"sum = {sum(a_b.values())}\",a_b)\n","  print(f\"sum = {sum(a_c.values())}\",a_c)\n","  print(f\"sum = {sum(b_c.values())}\",b_c)\n","  print(f\"sum = {-2 * sum(a_b_c.values())}\",a_b_c)\n","  print(f\"sum = {sum(abc.values())}\",abc)\n","  \n","  \n","  print(f\"ds_base = {s_ds} | ds_para = {p_ds}\")\n","  print(f\"bs_base = {s_bs} | bs_para = {p_bs}\")\n","\n","\n","  outlier_dict[str(outlier)] = {\n","      'reason':reason,\n","      'ds_scores': [s_ds, p_ds],\n","      'bs_scores': [s_bs, p_bs],\n","      'a_b': [\n","          a_b,\n","          sum(a_b.values()),\n","      ],\n","      'a_c': [\n","          a_c,\n","          sum(a_c.values()),\n","      ],\n","      'b_c': [\n","          b_c,\n","          sum(b_c.values()),\n","      ],\n","      'a_b_c': [\n","          a_b_c,\n","          sum(a_b_c.values()),\n","      ],\n","      'abc': [\n","          abc,\n","          sum(abc.values()),\n","      ],\n","      'sum(s_a_b, s_a_c, s_b_c)': sum(s_a_b.values()) + sum(s_a_c.values()) + sum(s_b_c.values()),\n","      's_a_b_c': sum(s_a_b_c.values()),\n","      's_abc': sum(s_abc.values()),\n","\n","  }\n","  outlier_dict[str(outlier)].update(\n","      {summ_types[idx]: [s[idx], p[idx].strip(\"\\n \")] for idx, summ in enumerate(s)}\n","  )\n","  print(json.dumps(outlier_dict, indent=1))\n","  return(outlier_dict)"],"metadata":{"id":"e7Pf72vPJ8bf","executionInfo":{"status":"ok","timestamp":1682059833641,"user_tz":240,"elapsed":109,"user":{"displayName":"Philip George","userId":"01883735276937380273"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["## READ DATAFRAMES\n","ds_ref_base = pd.read_csv(\"data/results/ds_ref_base.csv\")\n","ds_ref_para = pd.read_csv(\"data/results/ds_ref_paraphrase.csv\")\n","bs_ref_base = pd.read_csv(\"data/results/bs_ref_base.csv\")\n","bs_ref_para = pd.read_csv(\"data/results/bs_ref_paraphrase.csv\")\n","valid_paraphrase_examples = {}\n","# display(ds_ref_base.columns)\n","# print(ds_ref_base.loc[1]['summ_a'])\n","\n","## IDENTIFY VALID SUMMARIES PROGRAMMATICALLY\n","validity = []\n","for i in range(0, len(ds_ref_base)):\n","  # print()\n","  # print(f\"Example {i}\")\n","  summ_types = ['summ_a', 'summ_b', 'summ_comm']\n","  for summ_type in summ_types:\n","    # print(f\"Summ = {summ_type}\")\n","    original_summ = ds_ref_base.loc[i][summ_type].strip(\" \\n\")\n","    paraphrase_summ = ds_ref_para.loc[i][summ_type].strip(\" \\n\")\n","    if original_summ[-1] == '.' and paraphrase_summ[-1] != '.':\n","      # print(f\"invalid summ_type = {summ_type}\")\n","      # print(f\"ORIG = {original_summ[-100:]}\")\n","      # print(f\"PARA = {paraphrase_summ[-100:]}\")\n","      validity.append(False)\n","      break\n","  \n","  if len(validity) == i:\n","    # print(\"Summary was valid\")\n","    validity.append(True)\n","    original_summ = ds_ref_base.loc[i]['summ_comm'].strip(\" \\n\")\n","    paraphrase_summ = ds_ref_para.loc[i]['summ_comm'].strip(\" \\n\")\n","    # print(f\"ORIG = {original_summ[-100:]}\")\n","    # print(f\"PARA = {paraphrase_summ[-100:]}\")\n","  \n","  # print(f\"length(valid) = {len(validity)}\")\n","  \n","# Analyzed examples which were programmatically marked as invalid, and verified\n","# that they were complete\n","manually_verified_paraphrases = [9, 19, 33, 37, 47]\n","rule_verified_examples = np.where(np.array(validity)==1)[0].astype(np.int64).tolist()\n","\n","## CREATE VALID PARAPHRASE DATA\n","valid_paraphrase_examples = manually_verified_paraphrases + rule_verified_examples\n","valid_paraphrase_examples = np.array(sorted(valid_paraphrase_examples, reverse=False))\n","print(f\"valid_paraphrase_examples = {valid_paraphrase_examples}\")\n","print(f\"len(valid_paraphrase_examples) = {len(valid_paraphrase_examples)}\")\n","\n","\n","## SET HYPERPARAMETERS\n","NUM_OUTLIERS = 1\n","\n","## IDENTIFY DS OUTLIERS NOT PRESENT IN BS BASED ON DISTANCE ALONE\n","\n","x = np.array(ds_ref_base['ds_score'])\n","y = np.array(ds_ref_para['ds_score'])\n","m, c = np.polyfit(x, y, 1)\n","invalid_mask = np.ones(x.shape, bool)\n","invalid_mask[valid_paraphrase_examples] = False\n","dists = np.abs(y - m * x - c)/np.sqrt(1 + m ** 2)\n","dists[invalid_mask] = 0\n","# print(dists)\n","\n","ds_outliers = np.argsort(-dists)[:NUM_OUTLIERS]\n","print(ds_outliers)\n","\n","## PLOT DS(orig, para) with colors 'red' = invalid examples, \n","# 'blue' = valid_examples, 'green' = valid example outliers\n","fig, ax = plt.subplots(1)\n","colors = np.array(['r']*48)\n","colors[valid_paraphrase_examples] = 'b'\n","colors[ds_outliers] = 'g'\n","ax.scatter(x, y, color=colors)\n","ax.plot(x, m*x+c)\n","\n","## PLOT BS(orig, para) with colors 'red' = invalid examples, \n","# 'blue' = valid_examples, 'green' = valid example outliers\n","# as obtained from the DS regression model\n","x = np.array(bs_ref_base['bs_score'])\n","y = np.array(bs_ref_para['bs_score'])\n","m, c = np.polyfit(x, y, 1)\n","invalid_mask = np.ones(x.shape, bool)\n","invalid_mask[valid_paraphrase_examples] = False\n","dists = np.abs(y - m * x - c)/np.sqrt(1 + m ** 2)\n","dists[invalid_mask] = 0\n","# print(dists)\n","\n","bs_outliers = np.argsort(-dists)[:NUM_OUTLIERS]\n","print(bs_outliers)\n","\n","# outliers which are present in DS but not BS\n","exc_outliers = np.setdiff1d(ds_outliers, bs_outliers)\n","print(exc_outliers)\n","\n","# Purple in BS graph means an outlier from the DS graph\n","fig, ax = plt.subplots(1)\n","colors = np.array(['r']*48)\n","colors[valid_paraphrase_examples] = 'b'\n","colors[bs_outliers] = 'g'\n","colors[ds_outliers] = 'c'\n","colors[exc_outliers] = 'm'\n","ax.scatter(x, y, color=colors)\n","ax.plot(x, m*x+c)\n","\n","# Initialize dict to hold the error analysis examples\n","evaluator = get_evaluator()\n","\n","# ANALYZE EXCLUSIVE DS OUTLIERS\n","for outlier in exc_outliers:\n","\n","  s_ds = ds_ref_base.loc[outlier]['ds_score']\n","  p_ds = ds_ref_para.loc[outlier]['ds_score']\n","  s_bs = bs_ref_base.loc[outlier]['bs_score']\n","  p_bs = bs_ref_para.loc[outlier]['bs_score']\n","  summ_types = ['summ_a', 'summ_b', 'summ_comm']\n","  s, p = [], []\n","  for summ_type in summ_types:\n","    s.append(ds_ref_base.loc[outlier][summ_type])\n","    p.append(ds_ref_para.loc[outlier][summ_type])\n","  \n","  outlier_dict = perform_analysis(s, p, s_ds, p_ds, s_bs, p_bs, summ_types, 'Was outlier in scatter(ds_orig, ds_para) but not scatter(bs_orig, bs_para)')\n","  analysis_dict.update(outlier_dict)  \n","\n","## ANALYZE DS(orig, paraphrase) pairs with unusual differences\n"],"metadata":{"id":"QnY5CpZ2CjSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# HYPERPARAMETERS\n","SIG_LEVEL = 0.01\n","NUM_EXAMPLES = 2\n","\n","# CALCULATE differences\n","x = ds_ref_base['ds_score'].values*100\n","y = ds_ref_para['ds_score'].values*100\n","diffs = y - x\n","sample_mean = 0.0\n","sample_std = np.std(diffs, ddof=1)\n","\n","# Calculate t scores and set t scores of invalid examples to 0\n","# Also set t_scores < t_critical to 0 since they don't feature in our \n","# calculation anymore\n","# and sort in descending order of aboslute values\n","t_scores = (diffs - sample_mean) / (sample_std / np.sqrt(diffs.shape[0]))\n","invalid_mask = np.ones(diffs.shape, bool)\n","invalid_mask[valid_paraphrase_examples] = False\n","t_scores[invalid_mask] = 0\n","t_critical = stats.t.ppf(q=1-SIG_LEVEL/2,df=x.shape[0]-1)\n","t_scores[np.abs(t_scores)<=t_critical] = 0\n","diff_outliers = np.argsort(- np.abs(t_scores))[:NUM_EXAMPLES]\n","print(diff_outliers)\n","print(diff_outliers.shape[0])\n","# ANALYZE EXCLUSIVE DS OUTLIERS\n","for outlier in diff_outliers:\n","  s_ds = ds_ref_base.loc[outlier]['ds_score']\n","  p_ds = ds_ref_para.loc[outlier]['ds_score']\n","  s_bs = bs_ref_base.loc[outlier]['bs_score']\n","  p_bs = bs_ref_para.loc[outlier]['bs_score']\n","  summ_types = ['summ_a', 'summ_b', 'summ_comm']\n","  s, p = [], []\n","  for summ_type in summ_types:\n","    s.append(ds_ref_base.loc[outlier][summ_type])\n","    p.append(ds_ref_para.loc[outlier][summ_type])\n","  \n","  outlier_dict = perform_analysis(s, p, s_ds, p_ds, s_bs, p_bs, summ_types, 'Was outlier w.r.t other examples in terms of diff = (DS_para - DS_original)')\n","  analysis_dict.update(outlier_dict)  \n","\n","## ANALYZE DS(orig, paraphrase) pairs with unusual differences\n"],"metadata":{"id":"pGQJKFklDD1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WRITE ANALYSIS DICT TO FILE\n","json.dump(analysis_dict, open(\"data/results/error_analysis_ds_bs.json\",'w'))"],"metadata":{"id":"7BjVIUKtlggY","executionInfo":{"status":"ok","timestamp":1682060127241,"user_tz":240,"elapsed":139,"user":{"displayName":"Philip George","userId":"01883735276937380273"}}},"execution_count":59,"outputs":[]}]}